{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Guardar archivos en drive\n"
      ],
      "metadata": {
        "id": "LMYR43azGvLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_805HEdGv0-",
        "outputId": "b8a686f7-3034-4884-9bed-e20923bc4948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Cambia el nombre si quieres otra carpeta\n",
        "drive_folder = '/content/drive/MyDrive/Delfin'\n",
        "\n",
        "# Crear la carpeta si no existe\n",
        "os.makedirs(drive_folder, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Gmg6n-w6G5KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuración del entorno"
      ],
      "metadata": {
        "id": "mAl3PrERt2Om"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ffe3ceee",
        "outputId": "21a11fe1-3028-4382-f206-da73733471f9"
      },
      "source": [
        "!apt-get update && apt-get install -y sox libsox-dev libsox-fmt-all"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to cloud.r-project.or\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [2 InRelease 50.4 kB/128 kB 39%] [3 InRelease 117 kB/129 kB 91%] [Connected \r0% [2 InRelease 64.9 kB/128 kB 51%] [Connected to cloud.r-project.org (108.138.\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\r                                                                               \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,266 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,103 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [75.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,569 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,031 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,420 kB]\n",
            "Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,809 kB]\n",
            "Get:18 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,111 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,760 kB]\n",
            "Fetched 28.5 MB in 3s (10.5 MB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base libsox-fmt-mp3 libsox-fmt-oss\n",
            "  libsox-fmt-pulse libsox3 libwavpack1\n",
            "Suggested packages:\n",
            "  libaudio2 libsndio6.1\n",
            "The following NEW packages will be installed:\n",
            "  libao-common libao4 libid3tag0 libmad0 libopencore-amrnb0 libopencore-amrwb0\n",
            "  libsox-dev libsox-fmt-all libsox-fmt-alsa libsox-fmt-ao libsox-fmt-base\n",
            "  libsox-fmt-mp3 libsox-fmt-oss libsox-fmt-pulse libsox3 libwavpack1 sox\n",
            "0 upgraded, 17 newly installed, 0 to remove and 36 not upgraded.\n",
            "Need to get 1,157 kB of archives.\n",
            "After this operation, 4,262 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao-common all 1.2.2+20180113-1.1ubuntu3 [6,568 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libao4 amd64 1.2.2+20180113-1.1ubuntu3 [35.2 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libid3tag0 amd64 0.15.1b-14 [31.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmad0 amd64 0.15.1b-10ubuntu1 [63.1 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrnb0 amd64 0.1.5-1 [94.8 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopencore-amrwb0 amd64 0.1.5-1 [49.1 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [240 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [11.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-ao amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,740 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwavpack1 amd64 5.4.0-1build2 [83.7 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-base amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [33.7 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [17.3 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-oss amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [9,424 B]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-pulse amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [7,732 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-fmt-all amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [5,016 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsox-dev amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [356 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 sox amd64 14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1 [104 kB]\n",
            "Fetched 1,157 kB in 0s (8,333 kB/s)\n",
            "Selecting previously unselected package libao-common.\n",
            "(Reading database ... 126281 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libao-common_1.2.2+20180113-1.1ubuntu3_all.deb ...\n",
            "Unpacking libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libao4:amd64.\n",
            "Preparing to unpack .../01-libao4_1.2.2+20180113-1.1ubuntu3_amd64.deb ...\n",
            "Unpacking libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Selecting previously unselected package libid3tag0:amd64.\n",
            "Preparing to unpack .../02-libid3tag0_0.15.1b-14_amd64.deb ...\n",
            "Unpacking libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Selecting previously unselected package libmad0:amd64.\n",
            "Preparing to unpack .../03-libmad0_0.15.1b-10ubuntu1_amd64.deb ...\n",
            "Unpacking libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
            "Preparing to unpack .../04-libopencore-amrnb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
            "Preparing to unpack .../05-libopencore-amrwb0_0.1.5-1_amd64.deb ...\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Selecting previously unselected package libsox3:amd64.\n",
            "Preparing to unpack .../06-libsox3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
            "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-ao:amd64.\n",
            "Preparing to unpack .../08-libsox-fmt-ao_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libwavpack1:amd64.\n",
            "Preparing to unpack .../09-libwavpack1_5.4.0-1build2_amd64.deb ...\n",
            "Unpacking libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\n",
            "Preparing to unpack .../10-libsox-fmt-base_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
            "Preparing to unpack .../11-libsox-fmt-mp3_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-oss:amd64.\n",
            "Preparing to unpack .../12-libsox-fmt-oss_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-pulse:amd64.\n",
            "Preparing to unpack .../13-libsox-fmt-pulse_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-fmt-all:amd64.\n",
            "Preparing to unpack .../14-libsox-fmt-all_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libsox-dev:amd64.\n",
            "Preparing to unpack .../15-libsox-dev_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package sox.\n",
            "Preparing to unpack .../16-sox_14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-oss:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao-common (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libid3tag0:amd64 (0.15.1b-14) ...\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libao4:amd64 (1.2.2+20180113-1.1ubuntu3) ...\n",
            "Setting up libmad0:amd64 (0.15.1b-10ubuntu1) ...\n",
            "Setting up libwavpack1:amd64 (5.4.0-1build2) ...\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.5-1) ...\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-ao:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-mp3:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-pulse:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up sox (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-fmt-all:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Setting up libsox-dev:amd64 (14.4.2+git20190427-2+deb11u2ubuntu0.22.04.1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cc3Lba7Yz-M",
        "outputId": "32ae8d28-ea62-4c48-df57-990c25919f1e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastdtw) (2.0.2)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp311-cp311-linux_x86_64.whl size=542097 sha256=68db90fb209de960dcc9990131d7df40fe7885f01dcd81e84855fb30a51e750b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n"
          ]
        }
      ],
      "source": [
        "!pip install fastdtw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0yaPsl1Y3Yj",
        "outputId": "2ad1ecdd-90bf-4e63-deca-0467b1b4c194",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtw-python\n",
            "  Downloading dtw_python-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/48.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from dtw-python) (1.15.3)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from dtw-python) (2.0.2)\n",
            "Downloading dtw_python-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (801 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m801.7/801.7 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dtw-python\n",
            "Successfully installed dtw-python-1.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install dtw-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalación de dependencias"
      ],
      "metadata": {
        "id": "AL6cOYe4t_Mh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXnZNTrNYrM1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import output\n",
        "import base64\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocesamiento"
      ],
      "metadata": {
        "id": "DnGU5U8EuH1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls56DAPEZFFp"
      },
      "outputs": [],
      "source": [
        "# Parámetros\n",
        "TARGET_WORDS = [\"house\", \"right\", \"down\", \"go\"]\n",
        "word_to_idx = {w: i for i, w in enumerate(TARGET_WORDS)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "MAX_LEN = 64  # Longitud en frames del espectrograma\n",
        "\n",
        "# Transformación a espectrograma Mel\n",
        "mel_spec_transform = T.MelSpectrogram(\n",
        "    sample_rate=16000,\n",
        "    n_fft=2048, # Increased FFT size for better frequency resolution\n",
        "    hop_length=256, # Decreased hop length for better time resolution\n",
        "    n_mels=128 # Increased number of Mel bins for more frequency detail\n",
        ")\n",
        "\n",
        "# Preprocesamiento mejorado con recorte de silencio y ajuste de duración\n",
        "def preprocess(waveform, sr, trim=True, max_dur=1.0):\n",
        "    # Resamplear si es necesario\n",
        "    if sr != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
        "\n",
        "    # Recortar silencios al inicio y final\n",
        "    if trim:\n",
        "        waveform, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
        "            waveform, 16000,\n",
        "            effects=[\n",
        "                [\"silence\", \"1\", \"0.1\", \"1%\"],\n",
        "                [\"reverse\"],\n",
        "                [\"silence\", \"1\", \"0.1\", \"1%\"],\n",
        "                [\"reverse\"]\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Normalizar a duración fija (1 segundo)\n",
        "    target_len = int(16000 * max_dur)\n",
        "    current_len = waveform.shape[1]\n",
        "    if current_len > target_len:\n",
        "        waveform = waveform[:, :target_len]\n",
        "    elif current_len < target_len:\n",
        "        pad_len = target_len - current_len\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_len)) # Corrected padding\n",
        "\n",
        "    # Convertir a espectrograma Mel y escalar a dB\n",
        "    mel_spec = mel_spec_transform(waveform).squeeze(0)\n",
        "    mel_spec_db = T.AmplitudeToDB()(mel_spec)\n",
        "\n",
        "    # Ajustar a longitud fija en el eje tiempo\n",
        "    # We need to recalculate MAX_LEN based on the new hop_length and target_len\n",
        "    new_max_len = int(np.ceil(target_len / mel_spec_transform.hop_length))\n",
        "    if mel_spec_db.shape[1] < new_max_len:\n",
        "        mel_spec_db = torch.nn.functional.pad(mel_spec_db, (0, new_max_len - mel_spec_db.shape[1])) # Corrected padding\n",
        "    else:\n",
        "        mel_spec_db = mel_spec_db[:, :new_max_len]\n",
        "\n",
        "    return mel_spec_db"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filtrado de dataset"
      ],
      "metadata": {
        "id": "BAPNxDA8EUnt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usa1mvE7ZG9Z"
      },
      "outputs": [],
      "source": [
        "class FilteredSpeechCommands(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.data = []\n",
        "        for waveform, sr, label, *_ in dataset:\n",
        "            if label in TARGET_WORDS:\n",
        "                spec = preprocess(waveform, sr)\n",
        "                self.data.append((spec, word_to_idx[label]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definición red neuronal"
      ],
      "metadata": {
        "id": "Q-Q1-ILPuM-x"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiPkY-wRZIuR"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # The first convolution layer now expects 128 input channels (n_mels)\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4)) # Adaptive pooling handles different input sizes\n",
        "        # The input size to the first fully connected layer might change depending on the output of global_pool\n",
        "        # With AdaptiveAvgPool2d((4, 4)), the output size is fixed at 32*4*4\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, len(TARGET_WORDS))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamiento de la red neuronal (solo una vez)"
      ],
      "metadata": {
        "id": "nwNLt-jNuQHt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLAnVAr9ZKKw",
        "outputId": "b5f93ed6-0009-4ae5-fe18-d94ef6ad8895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Época 1, pérdida: 573.0260\n",
            "Época 2, pérdida: 400.9820\n",
            "Época 3, pérdida: 346.4965\n",
            "Época 4, pérdida: 302.6790\n",
            "Época 5, pérdida: 271.4754\n",
            "Época 6, pérdida: 248.0662\n",
            "Época 7, pérdida: 227.7377\n",
            "Época 8, pérdida: 214.8097\n",
            "Época 9, pérdida: 202.9443\n",
            "Época 10, pérdida: 196.1682\n"
          ]
        }
      ],
      "source": [
        "dataset_full = torchaudio.datasets.SPEECHCOMMANDS(\"./\", download=True)\n",
        "dataset_filtered = FilteredSpeechCommands(dataset_full)\n",
        "\n",
        "train_len = int(0.8 * len(dataset_filtered))\n",
        "train_set, test_set = random_split(dataset_filtered, [train_len, len(dataset_filtered) - train_len])\n",
        "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
        "\n",
        "model = CNNClassifier()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in train_loader:\n",
        "        preds = model(xb)\n",
        "        loss = loss_fn(preds, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Época {epoch+1}, pérdida: {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Función, detect voice activity\n"
      ],
      "metadata": {
        "id": "VWKxnyWs_iMe"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "433a8b52"
      },
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torchaudio.functional as F\n",
        "\n",
        "# Simple Voice Activity Detection (VAD) function\n",
        "def detect_voice_activity(waveform, sr, threshold=0.01, window_size=1024, hop_size=512):\n",
        "    \"\"\"\n",
        "    Detects voice activity in a waveform.\n",
        "    Returns the start and end sample indices of the voice segment.\n",
        "    \"\"\"\n",
        "    # Calculate Short-Time Energy\n",
        "    energy = torch.sum(waveform**2, dim=0)\n",
        "    energy_frames = energy.unfold(0, window_size, hop_size)\n",
        "    frame_energy = torch.log1p(torch.mean(energy_frames, dim=1)) # Use log1p for robustness\n",
        "\n",
        "    # Simple thresholding for VAD\n",
        "    active_frames = frame_energy > threshold\n",
        "    active_indices = torch.where(active_frames)[0]\n",
        "\n",
        "    if active_indices.numel() == 0:\n",
        "        return 0, waveform.shape[1] # No voice activity detected, return full range\n",
        "\n",
        "    start_frame = active_indices[0]\n",
        "    end_frame = active_indices[-1]\n",
        "\n",
        "    # Convert frame indices back to sample indices\n",
        "    start_sample = start_frame * hop_size\n",
        "    end_sample = min((end_frame * hop_size) + window_size, waveform.shape[1]) # Ensure end_sample does not exceed waveform length\n",
        "\n",
        "    return start_sample, end_sample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grabación de voz (1)"
      ],
      "metadata": {
        "id": "xNuYagFBuDQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "id": "F05lZ12UY_5o",
        "outputId": "22a55a6b-e2f1-4cf4-ce9a-63215a539b66"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<script>\n",
              "let div = document.createElement(\"div\");\n",
              "let p = document.createElement(\"p\");\n",
              "p.innerText = \"Graba tu voz diciendo una palabra (5 segundos)\";\n",
              "let btn = document.createElement(\"button\");\n",
              "btn.innerText = \"Grabar\";\n",
              "btn.style.padding = \"10px\";\n",
              "btn.onclick = () => {\n",
              "  navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n",
              "    const recorder = new MediaRecorder(stream);\n",
              "    let chunks = [];\n",
              "    recorder.ondataavailable = e => chunks.push(e.data);\n",
              "    recorder.onstop = () => {\n",
              "      const blob = new Blob(chunks);\n",
              "      const reader = new FileReader();\n",
              "      reader.onloadend = () => {\n",
              "        const base64Audio = reader.result.split(',')[1];\n",
              "        google.colab.kernel.invokeFunction('notebook.save_audio', [base64Audio], {});\n",
              "        p.innerText = \"Grabación completada\";\n",
              "      };\n",
              "      reader.readAsDataURL(blob);\n",
              "    };\n",
              "    recorder.start();\n",
              "    setTimeout(() => recorder.stop(), 1000);\n",
              "  });\n",
              "};\n",
              "div.appendChild(p);\n",
              "div.appendChild(btn);\n",
              "document.body.appendChild(div);\n",
              "</script>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio guardado.\n",
            "Audio guardado.\n",
            "Audio guardado.\n",
            "Audio guardado.\n"
          ]
        }
      ],
      "source": [
        "def save_audio(data):\n",
        "    with open(\"user.wav\", \"wb\") as f:\n",
        "        f.write(base64.b64decode(data))\n",
        "    print(\"Audio guardado.\")\n",
        "\n",
        "output.register_callback('notebook.save_audio', save_audio)\n",
        "\n",
        "RECORD_HTML = \"\"\"\n",
        "<script>\n",
        "let div = document.createElement(\"div\");\n",
        "let p = document.createElement(\"p\");\n",
        "p.innerText = \"Graba tu voz diciendo una palabra (5 segundos)\";\n",
        "let btn = document.createElement(\"button\");\n",
        "btn.innerText = \"Grabar\";\n",
        "btn.style.padding = \"10px\";\n",
        "btn.onclick = () => {\n",
        "  navigator.mediaDevices.getUserMedia({ audio: true }).then(stream => {\n",
        "    const recorder = new MediaRecorder(stream);\n",
        "    let chunks = [];\n",
        "    recorder.ondataavailable = e => chunks.push(e.data);\n",
        "    recorder.onstop = () => {\n",
        "      const blob = new Blob(chunks);\n",
        "      const reader = new FileReader();\n",
        "      reader.onloadend = () => {\n",
        "        const base64Audio = reader.result.split(',')[1];\n",
        "        google.colab.kernel.invokeFunction('notebook.save_audio', [base64Audio], {});\n",
        "        p.innerText = \"Grabación completada\";\n",
        "      };\n",
        "      reader.readAsDataURL(blob);\n",
        "    };\n",
        "    recorder.start();\n",
        "    setTimeout(() => recorder.stop(), 1000);\n",
        "  });\n",
        "};\n",
        "div.appendChild(p);\n",
        "div.appendChild(btn);\n",
        "document.body.appendChild(div);\n",
        "</script>\n",
        "\"\"\"\n",
        "display(HTML(RECORD_HTML))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consolidated cell for imports, functions, model definition, loading, and evaluation (2)\n"
      ],
      "metadata": {
        "id": "DVT1UlILF2Rb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34b3ef18",
        "outputId": "0b639488-457d-4fba-9258-a5b11424c6f1"
      },
      "source": [
        "# Consolidated cell for imports, functions, model definition, loading, and evaluation\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "from IPython.display import display, HTML\n",
        "from google.colab import output\n",
        "import base64\n",
        "import os\n",
        "\n",
        "# Parámetros (ensure these are defined or accessible)\n",
        "TARGET_WORDS = [\"house\", \"right\", \"down\", \"go\"]\n",
        "word_to_idx = {w: i for i, w in enumerate(TARGET_WORDS)}\n",
        "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
        "\n",
        "# Transformación a espectrograma Mel (ensure this is defined or accessible)\n",
        "mel_spec_transform = T.MelSpectrogram(\n",
        "    sample_rate=16000,\n",
        "    n_fft=2048,\n",
        "    hop_length=256,\n",
        "    n_mels=128\n",
        ")\n",
        "\n",
        "# Preprocesamiento mejorado con recorte de silencio y ajuste de duración\n",
        "def preprocess(waveform, sr, trim=True, max_dur=1.0):\n",
        "    # Resample if necessary\n",
        "    if sr != 16000:\n",
        "        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)(waveform)\n",
        "\n",
        "    # Trim silence at start and end\n",
        "    if trim:\n",
        "        try:\n",
        "            waveform, _ = torchaudio.sox_effects.apply_effects_tensor(\n",
        "                waveform, 16000,\n",
        "                effects=[\n",
        "                    [\"silence\", \"1\", \"0.1\", \"1%\"],\n",
        "                    [\"reverse\"],\n",
        "                    [\"silence\", \"1\", \"0.1\", \"1%\"],\n",
        "                    [\"reverse\"]\n",
        "                ]\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Silence trimming failed - {e}\")\n",
        "            # If trimming fails, use the original waveform\n",
        "            pass\n",
        "\n",
        "\n",
        "    # Normalize to fixed duration (1 second)\n",
        "    target_len = int(16000 * max_dur)\n",
        "    current_len = waveform.shape[1]\n",
        "    if current_len > target_len:\n",
        "        waveform = waveform[:, :target_len]\n",
        "    elif current_len < target_len:\n",
        "        pad_len = target_len - current_len\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, pad_len))\n",
        "\n",
        "    # Convert to Mel spectrogram and scale to dB\n",
        "    mel_spec = mel_spec_transform(waveform).squeeze(0)\n",
        "    mel_spec_db = T.AmplitudeToDB()(mel_spec)\n",
        "\n",
        "    # Adjust to fixed length on the time axis\n",
        "    new_max_len = int(np.ceil(target_len / mel_spec_transform.hop_length))\n",
        "    if mel_spec_db.shape[1] < new_max_len:\n",
        "        mel_spec_db = torch.nn.functional.pad(mel_spec_db, (0, new_max_len - mel_spec_db.shape[1]))\n",
        "    else:\n",
        "        mel_spec_db = mel_spec_db[:, :new_max_len]\n",
        "\n",
        "    return mel_spec_db\n",
        "\n",
        "# Simple Voice Activity Detection (VAD) function\n",
        "def detect_voice_activity(waveform, sr, threshold=0.01, window_size=1024, hop_size=512):\n",
        "    \"\"\"\n",
        "    Detects voice activity in a waveform.\n",
        "    Returns the start and end sample indices of the voice segment.\n",
        "    \"\"\"\n",
        "    # Ensure waveform is 1D for energy calculation if it's 2D (channels, samples)\n",
        "    if waveform.ndim > 1:\n",
        "        # Assuming mono or first channel if stereo\n",
        "        waveform = waveform[0, :]\n",
        "\n",
        "    # Calculate Short-Time Energy\n",
        "    # Use a small epsilon to avoid log of zero\n",
        "    energy = waveform**2 + 1e-8\n",
        "    energy_frames = energy.unfold(0, window_size, hop_size)\n",
        "    frame_energy = torch.log1p(torch.mean(energy_frames, dim=1)) # Use log1p for robustness\n",
        "\n",
        "    # Simple thresholding for VAD\n",
        "    active_frames = frame_energy > threshold\n",
        "    active_indices = torch.where(active_frames)[0]\n",
        "\n",
        "    if active_indices.numel() == 0:\n",
        "        # No voice activity detected, return the full range of the original waveform\n",
        "        return 0, waveform.shape[0]\n",
        "\n",
        "    start_frame = active_indices[0]\n",
        "    end_frame = active_indices[-1]\n",
        "\n",
        "    # Convert frame indices back to sample indices\n",
        "    start_sample = start_frame * hop_size\n",
        "    end_sample = min((end_frame * hop_size) + window_size, waveform.shape[0]) # Ensure end_sample does not exceed waveform length\n",
        "\n",
        "    return start_sample, end_sample\n",
        "\n",
        "\n",
        "# CNN Classifier Model Definition\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(32)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.global_pool = nn.AdaptiveAvgPool2d((4, 4))\n",
        "        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
        "        self.fc2 = nn.Linear(128, len(TARGET_WORDS))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.global_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "# --- Model Loading and Evaluation Logic ---\n",
        "\n",
        "# Define the path where the model is saved in Google Drive\n",
        "model_save_path = \"/content/drive/MyDrive/SpeechProject/cnn_classifier_model.pth\"\n",
        "\n",
        "# Instantiate a new model and optimizer (they need to be defined first)\n",
        "loaded_model = CNNClassifier()\n",
        "loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=0.001) # Use the same optimizer and learning rate as during training\n",
        "\n",
        "# Check if the saved model file exists and load it\n",
        "if os.path.exists(model_save_path):\n",
        "    # Load the saved state\n",
        "    checkpoint = torch.load(model_save_path)\n",
        "\n",
        "    # Load the state dictionaries into the model and optimizer\n",
        "    loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    print(f\"Modelo cargado desde {model_save_path}\")\n",
        "\n",
        "    # Set the model to evaluation mode\n",
        "    loaded_model.eval()\n",
        "\n",
        "    # Load the user's recorded audio file\n",
        "    waveform_user, sr_user = torchaudio.load(\"user.wav\")\n",
        "\n",
        "    # Check for voice activity\n",
        "    start_sample_user, end_sample_user = detect_voice_activity(waveform_user, sr_user)\n",
        "\n",
        "    if start_sample_user == 0 and end_sample_user == waveform_user.shape[-1]:\n",
        "         # This condition might need refinement depending on desired VAD sensitivity\n",
        "         # A simple check for low energy might be more appropriate for \"silence detected\"\n",
        "         # For now, we rely on the VAD function returning full range if no activity above threshold\n",
        "         # We can add an explicit check for low overall energy if needed.\n",
        "         overall_energy = torch.sum(waveform_user**2)\n",
        "         if overall_energy < 0.05: # Example threshold for low energy\n",
        "             print(\"\\nSilencio o ruido detectado. No se reconoció ninguna palabra.\")\n",
        "             recognized_word = \"SILENCE/NOISE\" # Assign a value to recognized_word\n",
        "         else:\n",
        "             # Process if VAD returned full range but overall energy is not low\n",
        "             spec_user = preprocess(waveform_user, sr_user).unsqueeze(0)\n",
        "             with torch.no_grad():\n",
        "                 logits = loaded_model(spec_user)\n",
        "                 probabilities = F.softmax(logits, dim=1)\n",
        "                 predicted_label_index = torch.argmax(probabilities, dim=1).item()\n",
        "                 predicted_probability = probabilities[0, predicted_label_index].item()\n",
        "                 recognized_word = idx_to_word[predicted_label_index]\n",
        "\n",
        "             confidence_threshold = 0.7\n",
        "             print(f\"\\nProbabilidad de '{recognized_word.upper()}': {predicted_probability:.4f}\")\n",
        "             if predicted_probability < confidence_threshold:\n",
        "                 print(\"La palabra no fue reconocida con alta confianza.\")\n",
        "             else:\n",
        "                 print(f\"Palabra reconocida: {recognized_word.upper()}\")\n",
        "\n",
        "\n",
        "    else:\n",
        "        # If voice activity is detected and VAD returned a sub-segment\n",
        "        # Crop the waveform to the detected voice activity segment for preprocessing\n",
        "        waveform_user_active = waveform_user[:, start_sample_user:end_sample_user]\n",
        "        spec_user = preprocess(waveform_user_active, sr_user).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = loaded_model(spec_user)\n",
        "            probabilities = F.softmax(logits, dim=1)\n",
        "            predicted_label_index = torch.argmax(probabilities, dim=1).item()\n",
        "            predicted_probability = probabilities[0, predicted_label_index].item()\n",
        "            recognized_word = idx_to_word[predicted_label_index]\n",
        "\n",
        "        confidence_threshold = 0.7\n",
        "        print(f\"\\nProbabilidad de '{recognized_word.upper()}': {predicted_probability:.4f}\")\n",
        "        if predicted_probability < confidence_threshold:\n",
        "            print(\"La palabra no fue reconocida con alta confianza.\")\n",
        "        else:\n",
        "            print(f\"Palabra reconocida: {recognized_word.upper()}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(f\"Archivo de modelo no encontrado en {model_save_path}. Entrena el modelo primero.\")\n",
        "    recognized_word = \"MODEL_NOT_FOUND\" # Assign a value to recognized_word to prevent NameError later"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo cargado desde /content/drive/MyDrive/SpeechProject/cnn_classifier_model.pth\n",
            "\n",
            "Probabilidad de 'RIGHT': 0.9628\n",
            "Palabra reconocida: RIGHT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear archivo .wav de referencia (3)"
      ],
      "metadata": {
        "id": "I_MQD4s-uWFq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avs1a_XtcKmv",
        "outputId": "d23aa6b7-6820-40e0-9ea3-b0f548317d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intentando cargar el dataset desde el path local: ./\n",
            "Dataset cargado desde el path local.\n",
            "Archivo ref_right.wav guardado en el path local.\n"
          ]
        }
      ],
      "source": [
        "# Define potential paths for the dataset\n",
        "dataset_local_path = \"./\"\n",
        "# Assuming the dataset might be directly in SpeechProject or a subfolder like 'SpeechCommands' within it\n",
        "dataset_drive_path_option1 = \"/content/drive/MyDrive/SpeechProject/\"\n",
        "dataset_drive_path_option2 = \"/content/drive/MyDrive/SpeechProject/SpeechCommands/\"\n",
        "\n",
        "\n",
        "dataset_full = None # Initialize dataset_full to None\n",
        "\n",
        "print(f\"Intentando cargar el dataset desde el path local: {dataset_local_path}\")\n",
        "try:\n",
        "    # Try loading from the local path first (where it might have been downloaded)\n",
        "    dataset_full = torchaudio.datasets.SPEECHCOMMANDS(dataset_local_path, download=False)\n",
        "    print(\"Dataset cargado desde el path local.\")\n",
        "except Exception as e_local:\n",
        "    print(f\"Error cargando dataset desde el path local: {e_local}\")\n",
        "    print(f\"Intentando cargar el dataset desde Google Drive (Option 1): {dataset_drive_path_option1}\")\n",
        "    # If local loading fails, attempt to load from Google Drive (Option 1)\n",
        "    try:\n",
        "        dataset_full = torchaudio.datasets.SPEECHCOMMANDS(dataset_drive_path_option1, download=False)\n",
        "        print(\"Dataset cargado desde Google Drive (Option 1).\")\n",
        "    except Exception as e_drive1:\n",
        "        print(f\"Error cargando dataset desde Google Drive (Option 1): {e_drive1}\")\n",
        "        print(f\"Intentando cargar el dataset desde Google Drive (Option 2): {dataset_drive_path_option2}\")\n",
        "        # If Option 1 fails, attempt to load from Google Drive (Option 2)\n",
        "        try:\n",
        "            dataset_full = torchaudio.datasets.SPEECHCOMMANDS(dataset_drive_path_option2, download=False)\n",
        "            print(\"Dataset cargado desde Google Drive (Option 2).\")\n",
        "        except Exception as e_drive2:\n",
        "            print(f\"Error cargando dataset desde Google Drive (Option 2): {e_drive2}\")\n",
        "            print(\"El dataset no fue encontrado en ninguno de los paths especificados.\")\n",
        "            print(\"Por favor, asegúrate de que el dataset esté descargado y extraído en '/content/' o en tu Google Drive en '/content/drive/MyDrive/SpeechProject/' o '/content/drive/MyDrive/SpeechProject/SpeechCommands/'.\")\n",
        "\n",
        "\n",
        "# Crear archivo de referencia ref_<palabra>.wav si no existe\n",
        "def save_reference_sample(label):\n",
        "    # Ensure dataset_full is loaded before proceeding\n",
        "    if dataset_full is None:\n",
        "        print(\"No se pudo crear el archivo de referencia porque el dataset no está cargado.\")\n",
        "        return\n",
        "\n",
        "    for waveform, sr, l, *_ in dataset_full:\n",
        "        if l == label:\n",
        "            # Define the path to save the reference file in the current directory (where user.wav is saved)\n",
        "            reference_save_path = f\"ref_{label}.wav\"\n",
        "            torchaudio.save(reference_save_path, waveform, sr)\n",
        "            print(f\"Archivo {f'ref_{label}.wav'} guardado en el path local.\")\n",
        "            return\n",
        "    print(f\"No se encontró una muestra para la palabra '{label}' en el dataset.\")\n",
        "\n",
        "# Usar palabra reconocida como referencia\n",
        "# Check if recognized_word is defined and is not \"MODEL_NOT_FOUND\" or \"SILENCE/NOISE\" and dataset is loaded\n",
        "if 'recognized_word' in locals() and recognized_word not in [\"MODEL_NOT_FOUND\", \"SILENCE/NOISE\"] and dataset_full is not None:\n",
        "    save_reference_sample(recognized_word)\n",
        "else:\n",
        "    print(\"No se pudo crear el archivo de referencia. La palabra reconocida no es válida o el modelo/dataset no fue cargado correctamente.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}